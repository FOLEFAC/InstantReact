{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms # All torchvision modules\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, Loss functions,..\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam,...\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from torch.utils.data import (DataLoader,Dataset)  # Gives easier dataset managment and creates mini batches\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "import torchtext # Makes it easy to work with sequence data \n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "import re # regex library\n",
    "import os # Doing operating system operations\n",
    "import cv2 # Computer vision tasks with OpenCV\n",
    "import numpy as np # Powerful arrray computation library\n",
    "from PIL import Image # WOrking with image files\n",
    "import pandas # Extracting data from csv\n",
    "import math # Math package\n",
    "import pickle # Saving variables for later usage.\n",
    "\n",
    "from torchsummary import summary # Make understanding of models easier\n",
    "import torch # PyTorch library\n",
    "from time import time # Using timer in code\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use Cuda if GPU available!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device) # Test for device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    '''\n",
    "    This class contains methods which help in processing our dataset\n",
    "    Args: No arguments\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def output_text(self, train_corpus, video = None):\n",
    "        \n",
    "        '''\n",
    "        Purpose: Generate all text present in video using the csv file which contains all the text in the videos\n",
    "        Input(s):\n",
    "            train_corpus: The file path to the csv file\n",
    "            video: A video file whose text is to be generated\n",
    "        Outputs(s):\n",
    "            Final description: the text representing a video caption\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        df = pandas.read_csv(train_corpus)\n",
    "        if (video):\n",
    "            video_id,start,end = self.get_video_id(video) # \n",
    "\n",
    "            final_description=''\n",
    "            for i in range(len(df)):\n",
    "\n",
    "                if df['VideoID'][i]==str(video_id) and df['Start'][i]==int(start) and df['End'][i]==int(end) and df['Language'][i]=='English':\n",
    "\n",
    "                    final_description=df['Description'][i]\n",
    "        else:\n",
    "            \n",
    "            final_description = []\n",
    "            for i in range(len(df)):\n",
    "                if (df['Language'][i]=='English'):\n",
    "                    final_description.append(df['Description'][i])\n",
    "        return final_description\n",
    "            \n",
    "\n",
    "    def get_video_id(self, video_path):\n",
    "            \n",
    "        '''\n",
    "        Purpose: Extract video name (without extension) and also remove the start and end tags from the video file name\n",
    "        Input(s): video file path EX: videoname_xx_yy.avi\n",
    "        Outputs(s): extracted videoname, xx = start tag, yy=end tag\n",
    "        \n",
    "        '''\n",
    "        video_id=None\n",
    "        start=None\n",
    "        end=None\n",
    "        video_path=video_path[0:len(video_path)-4]\n",
    "        counter=0\n",
    "        for i in reversed(range(len(video_path))):\n",
    "            if (video_path[i]=='_' and counter<2):\n",
    "\n",
    "                if (counter == 0):\n",
    "                    end=video_path[i+1:]\n",
    "                    video_path=video_path[0:i]\n",
    "                else:\n",
    "                    start=video_path[i+1:]\n",
    "                    video_path=video_path[0:i]\n",
    "                counter+=1\n",
    "        video_id=video_path\n",
    "\n",
    "        return video_id,start,end\n",
    "    @staticmethod\n",
    "    def tagger_input(text):    \n",
    "            \n",
    "        '''\n",
    "        Purpose: Add the beginning of sentence tag on a text\n",
    "        Input(s): \n",
    "            text: a String which represents a sentence from a video\n",
    "        Outputs(s): \n",
    "            text: A tagged String\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        bos=\"<bos> \"\n",
    "        text= bos+text \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def tagger_output(text):  \n",
    "           \n",
    "        '''\n",
    "        Purpose: Add the end of sentence tag on a text\n",
    "        Input(s): \n",
    "            text: a String which represents a sentence from a video\n",
    "        Outputs(s): \n",
    "            text: A tagged String\n",
    "        \n",
    "        '''\n",
    "        eos=\" <eos>\"\n",
    "        text= text+eos\n",
    "        \n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(texts):\n",
    "            \n",
    "        '''\n",
    "        Purpose:Clean text by removing unnecessary characters and altering the format of words.\n",
    "        Input(s):\n",
    "            texts: Texts which contain several symbols which aren't used by our model\n",
    "        Outputs(s):\n",
    "            texts: Texts which have been cleaned\n",
    "        \n",
    "        '''\n",
    "        for i in range(len(texts)):\n",
    "\n",
    "            if(texts==\"Commands[195]part4 of 9\"):\n",
    "                texts=\"commands 195 part 4 of 9\"\n",
    "\n",
    "            texts = texts.lower()\n",
    "            texts = re.sub(r\"i'm\", \"i am\", texts)\n",
    "            texts = re.sub(r\"he's\", \"he is\", texts)\n",
    "            texts = re.sub(r\"she's\", \"she is\", texts)\n",
    "            texts = re.sub(r\"it's\", \"it is\", texts)\n",
    "            texts = re.sub(r\"that's\", \"that is\", texts)\n",
    "            texts = re.sub(r\"what's\", \"that is\", texts)\n",
    "            texts = re.sub(r\"where's\", \"where is\", texts)\n",
    "            texts = re.sub(r\"how's\", \"how is\", texts)\n",
    "            texts = re.sub(r\"\\'ll\", \" will\", texts)\n",
    "            texts = re.sub(r\"\\'ve\", \" have\", texts)\n",
    "            texts = re.sub(r\"\\'re\", \" are\", texts)\n",
    "            texts = re.sub(r\"\\'d\", \" would\", texts)\n",
    "            texts = re.sub(r\"\\'re\", \" are\", texts)\n",
    "            texts = re.sub(r\"won't\", \"will not\", texts)\n",
    "            texts = re.sub(r\"\\n\",\"\",texts)\n",
    "            texts = re.sub(r\"\\r\",\"\",texts)\n",
    "            texts = re.sub(r\"_\",\" \",texts)\n",
    "            texts = re.sub(r\"can't\", \"cannot\", texts)\n",
    "            texts = re.sub(r\"n't\", \" not\", texts)\n",
    "            texts = re.sub(r\"n'\", \"ng\", texts)\n",
    "            texts = re.sub(r\"'bout\", \"about\", texts)\n",
    "            texts = re.sub(r\"'til\", \"until\", texts)\n",
    "            texts = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,&]\", \"\", texts)\n",
    "\n",
    "        return texts\n",
    "    \n",
    "    def video_to_frames(self, video_path,frame_number, device, INPUT_SIZE , model, transform):\n",
    "         \n",
    "        '''\n",
    "        Purpose: Take a video file and produce coded frames out of it\n",
    "        Input(s):\n",
    "            video_path: The video file to be processed\n",
    "            frame_number: The number of frames we want to extract (In our example, it is 40)\n",
    "            device: The device on which the inference will be done\n",
    "            INPUT_SIZE: The dimension of the output array of each frame\n",
    "            model: The CNN Model used for inference\n",
    "            transform: The transform object, which will process all images before they are passed to the model\n",
    "        Outputs(s):\n",
    "            The coded frames of dimension frame_number X  INPUT_SIZE (Ex: 40 X 2850)\n",
    "        \n",
    "        '''\n",
    "        cap=cv2.VideoCapture(video_path) # read the video file\n",
    "        number_of_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) # get the number of frames\n",
    "        get_filter=int(number_of_frames/frame_number) # obtain the factor of video number of frames to the number of frames\n",
    "        #we want to extract, so that There is equal spacing between the frames which make up the videos \n",
    "        \n",
    "        current_frame=0\n",
    "        total_features = torch.zeros([frame_number, INPUT_SIZE]) # initialize the total_features \n",
    "        total_features.to(dtype = torch.float16)\n",
    "        t=0\n",
    "        while (current_frame<number_of_frames):\n",
    "            ret,frame = cap.read()\n",
    "            \n",
    "            if ((current_frame%get_filter) == 0 and t<frame_number):\n",
    "                with torch.no_grad(): \n",
    "                    \n",
    "                    cv2_im = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB) # read the image using OpenCV library\n",
    "                    frame = Image.fromarray(cv2_im)\n",
    "\n",
    "                    frame = transform(frame) # Use transform to process the image before inference\n",
    "                    \n",
    "                    frame = frame.to(device) # set frame to be inferred using the set device\n",
    "                    model = model.to(device) # set model to infer using the set device\n",
    "                    \n",
    "                    model.eval() # put model in evaluation mode\n",
    "\n",
    "                    frame_feature = model(frame[None])\n",
    "                    \n",
    "                    frame_feature = torch.squeeze(frame_feature,0)\n",
    "                    \n",
    "                    total_features[t] = frame_feature\n",
    "\n",
    "                \n",
    "                t+=1\n",
    "            current_frame+=1\n",
    "            \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        return total_features\n",
    "\n",
    "    def get_pre_data(self, train_dir, frame_number, INPUT_SIZE, model , transform ):\n",
    "         \n",
    "        '''\n",
    "        Purpose: Could be used to obtain the coded frames, and stored in a pickle file, such that training can be faster\n",
    "        Input(s): \n",
    "            train_dir: The directory containing all  video files to be processed\n",
    "            frame_number: The number of frames we want to extract (In our example, it is 40)\n",
    "            INPUT_SIZE: The dimension of the output array of each frame\n",
    "            model: The CNN Model used for inference\n",
    "            transform: The transform object, which will process all images before they are passed to the model\n",
    "        Outputs(s):\n",
    "            All the coded frames in one output\n",
    "        \n",
    "        '''\n",
    "        print(train_dir)\n",
    "        train_video_list=os.listdir(train_dir) # get list of all files in the train directory\n",
    "        i=0\n",
    "        all_output = torch.zeros([len(train_video_list), frame_number, INPUT_SIZE])\n",
    "        \n",
    "        for video_path in train_video_list:\n",
    "            \n",
    "            video_path=train_dir+video_path\n",
    "            \n",
    "            output=self.video_to_frames(video_path,frame_number, 'cuda', INPUT_SIZE, model, transform)\n",
    "            \n",
    "            all_output[i] = output\n",
    "            i += 1\n",
    "            \n",
    "        return all_output\n",
    "\n",
    "    \n",
    "class TextProcessor:\n",
    "    '''\n",
    "    This class contains methods which help in processing text data\n",
    "    Args: \n",
    "        freq_threshold: Get the maximum frequency above which a word is not considered to be part of our vocabulary\n",
    "        VOCAB_SIZE: the vocabulary size\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, freq_threshold = None, VOCAB_SIZE = None):\n",
    "        \n",
    "        self.word_to_index = {\"<unk>\":0, \"<pad>\":1, \"<bos>\": 2, \"<eos>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.VOCAB_SIZE = VOCAB_SIZE\n",
    "        self.get_tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.itos)\n",
    "\n",
    "    def get_output(self, sentence_to_indices, NUMBER_OF_WORDS):\n",
    "         \n",
    "        '''\n",
    "        Purpose: Generate one - hot representation of sentence, ready for model training\n",
    "        Input(s): \n",
    "            sentence_to_indices: A dictionary which contains the words and indices as key, value pairs\n",
    "            NUMBER_OF_WORDS: The maximum number of words a sentence can contain\n",
    "        Outputs(s):\n",
    "            One-hot vectors stacked into an array\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        arr = np.zeros((NUMBER_OF_WORDS, self.VOCAB_SIZE))\n",
    "        pad_number = 1 # The pad in sentence to index is seen as 1\n",
    "        for i in range(len(arr)):\n",
    "            if(i<len(sentence_to_indices)):\n",
    "                arr[i][sentence_to_indices[list(sentence_to_indices.keys())[i]]] = 1 # set a given key to 1, while leaving the others at zero\n",
    "            else:\n",
    "                arr[i][pad_number] = 1 # pad to complete the remaining words to make up the NUMBER OF WORDS needed for the model\n",
    "                \n",
    "        return arr\n",
    "    def sentence_to_indices(self, sentence, dictionary):\n",
    "         \n",
    "        '''\n",
    "        Purpose: Take an input sentence and convert it to a dictionary which has words and their corresponding indices in the vocabulary as key, value pairs\n",
    "        Input(s):\n",
    "            sentence: The sentence whose words have to be linked to indices\n",
    "            dictionary: The dictionary which will contain the word to indices\n",
    "        Outputs(s):\n",
    "            sentence_to_index: word to index pair dictionary\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        sentence_to_index = {}\n",
    "        \n",
    "        tokenizer = self.get_tokenizer \n",
    "        for word in tokenizer(sentence):# go tgrough all the words formed after tokenizing the sentence\n",
    "            try:\n",
    "                if dictionary[word] < self.VOCAB_SIZE: # if word is part of vocabulary\n",
    "\n",
    "                    sentence_to_index[word] = dictionary[word] \n",
    "                else: # else it isn't added to the sentence_to_index\n",
    "                    continue\n",
    "            except:\n",
    "                sentence_to_index[word] = 0 # in case the word isn't found in the dictionary, we consider it to be unknown\n",
    "        return sentence_to_index\n",
    "    \n",
    "    \n",
    "    def vocab_creator(self,sentence_list):\n",
    "         \n",
    "        '''\n",
    "        Purpose: From a give corpus, generate a WORD vocabulary which maps a givenn word to a given index\n",
    "        Input(s): \n",
    "            sentence_list: A corpus of all sentences extracted from the videos in the dataset\n",
    "        Outputs(s):\n",
    "            word_to_index: the word to index of all words contained in the textual corpus\n",
    "        \n",
    "        '''\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        stoi = {}\n",
    "\n",
    "        tokenizer = self.get_tokenizer\n",
    "        for sentence in sentence_list:\n",
    "            try:\n",
    "                for word in tokenizer(sentence):\n",
    "                    if word not in frequencies:\n",
    "                        frequencies[word] = 1\n",
    "\n",
    "                    else:\n",
    "                        frequencies[word] += 1\n",
    "\n",
    "                    if frequencies[word] == self.freq_threshold:\n",
    "                        self.word_to_index[word] = idx\n",
    "                        idx += 1\n",
    "            except:\n",
    "                pass \n",
    "        return self.word_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train_dir, train_corpus, device, dictionary, VOCAB_SIZE, NUMBER_OF_WORDS, INPUT_SIZE, number_of_frames, transform, model = None, pre_data = None):\n",
    "        \n",
    "        self.train_dir = train_dir\n",
    "        self.train_dir_list = os.listdir(train_dir)\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        self.number_of_frames = number_of_frames\n",
    "        self.utils = Utils()\n",
    "        self.word_to_index = dictionary\n",
    "        self.VOCAB_SIZE = VOCAB_SIZE\n",
    "        self.NUMBER_OF_WORDS = NUMBER_OF_WORDS\n",
    "        self.INPUT_SIZE = INPUT_SIZE\n",
    "        self.pre_data = pre_data\n",
    "        self.device = device\n",
    "        self.train_corpus = train_corpus\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_dir_list)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        textprocessor = TextProcessor(VOCAB_SIZE = self.VOCAB_SIZE)\n",
    "        utils = Utils()\n",
    "        \n",
    "        \n",
    "        video_file = self.train_dir_list[idx] # get video file corresponding to the id, idx\n",
    "        \n",
    "        \n",
    "        output_text = self.utils.output_text(self.train_corpus, video_file) # get the text contained in the video file\n",
    "        \n",
    "        \n",
    "        #### generate input 2,  from the output_text\n",
    "        sentence_to_index = textprocessor.sentence_to_indices(utils.tagger_input(utils.clean_text(output_text)), self.word_to_index)\n",
    "        X_2 = textprocessor.get_output(sentence_to_index, NUMBER_OF_WORDS)\n",
    "        \n",
    "        #### generate output,  from the output_text\n",
    "        sentence_to_index = textprocessor.sentence_to_indices(utils.tagger_output(utils.clean_text(output_text)), self.word_to_index) \n",
    "        y = textprocessor.get_output(sentence_to_index, NUMBER_OF_WORDS)\n",
    "        \n",
    "        video_path = self.train_dir + video_file\n",
    "        \n",
    "        # generate input 1\n",
    "        X_1 = utils.video_to_frames(video_path, self.number_of_frames, self.device, self.INPUT_SIZE, self.model, self.transform)\n",
    "        #X_1 = pre_data[idx]\n",
    "        return (X_1,torch.tensor(X_2)), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = models.vgg16(pretrained=True)# obtain pretrained VGG16 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.classifier = nn.Sequential(*list(model_vgg.classifier.children())[:-2]) # remove last linear layer of VGG16 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parametres\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "NUMBER_OF_FRAMES = 40\n",
    "BATCH_SIZE = 1\n",
    "EPOCH = 10\n",
    "TRAINING_DEVICE = 'cuda'\n",
    "VOCAB_SIZE = 200\n",
    "NUMBER_OF_WORDS = 10\n",
    "HIDDEN_SIZE = 300\n",
    "INPUT_SIZE = 4096\n",
    "NUMBER_OF_LAYERS = 1\n",
    "tsfm = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_dir = 'D:/Machine_Learning/datasets/YouTubeClips_2/YouTubeClips/'\n",
    "train_corpus = 'D:/Machine_Learning/datasets/video_corpus/video_corpus.csv'\n",
    "utils = Utils()\n",
    "all_text = utils.output_text(train_corpus)\n",
    "text_processor = TextProcessor(freq_threshold = 10)\n",
    "dictionary = text_processor.vocab_creator(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training data preparation\n",
    "train_ds = CustomDataset(train_dir, train_corpus, device, dictionary, VOCAB_SIZE, NUMBER_OF_WORDS, INPUT_SIZE,  NUMBER_OF_FRAMES, tsfm, model = model_vgg)\n",
    "train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder_LSTM(\n",
      "    (lstm): LSTM(4096, 300, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder_LSTM(\n",
      "    (lstm): LSTM(200, 300, batch_first=True)\n",
      "    (fc): Linear(in_features=300, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sequence to sequence model\n",
    "\n",
    "class Encoder_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, c_n) = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "                                    \n",
    "        return h_n, c_n\n",
    "    \n",
    "class Decoder_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, number_of_words):\n",
    "        super(Decoder_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "    def forward(self, x, h_n, c_n):\n",
    "        output, _ = self.lstm(x.float(),(h_n,c_n))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        output = self.fc(output)                            \n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, X_1, X_2):\n",
    "        h_n, c_n = self.encoder(X_1)\n",
    "        output = self.decoder(X_2, h_n, c_n)\n",
    "        return output\n",
    "    \n",
    "encoder = Encoder_LSTM(input_size = INPUT_SIZE, hidden_size = HIDDEN_SIZE , num_layers = NUMBER_OF_LAYERS)\n",
    "decoder = Decoder_LSTM(input_size = VOCAB_SIZE, hidden_size = HIDDEN_SIZE , num_layers = NUMBER_OF_LAYERS, number_of_words = NUMBER_OF_WORDS)\n",
    "model_seq_to_seq = Seq2Seq(encoder, decoder).to(device)\n",
    "model = model_seq_to_seq\n",
    "print(model)\n",
    "### load the state_dict of model if model has been pretrained.\n",
    "model.load_state_dict(torch.load('model_lstm_best_loss.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Machine_Learning/datasets/YouTubeClips_2/YouTubeClips/-4wsuPCjDBc_5_15.avi\n",
      "when we getfilter = : 7\n",
      "epoch: 1 \tstep: 1 / 1001 \ttrain loss: 0.9817 \ttime: 1.2964 s\n",
      "D:/Machine_Learning/datasets/YouTubeClips_2/YouTubeClips/-7KMZQEsJW4_205_208.avi\n",
      "when we getfilter = : 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e5993ae1e562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-f78be95783cc>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0moutput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get the text contained in the video file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3407e041c810>\u001b[0m in \u001b[0;36moutput_text\u001b[1;34m(self, train_corpus, video)\u001b[0m\n\u001b[0;32m     20\u001b[0m         '''\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mvideo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_video_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m     \"\"\"\n\u001b[0;32m    544\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Model Training\n",
    "\n",
    "\n",
    "\n",
    "EPOCH = 10\n",
    "import time\n",
    "print_feq = 100\n",
    "best_loss = np.inf\n",
    "for epoch in range(1, EPOCH+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for step, (img,label) in enumerate(train_dl):\n",
    "        \n",
    "        \n",
    "        time_1 = time.time() ## timing\n",
    "        \n",
    "        X_1, X_2 = img ### get inputs\n",
    "        \n",
    "        X_1 = X_1.to(device) # Set device \n",
    "        X_2 = X_2.to(device) # Set device\n",
    "        \n",
    "        \n",
    "        label = label.to(device) # Set output device\n",
    "        \n",
    "        ### zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### forward\n",
    "        prediction = model(X_1, X_2)\n",
    "        \n",
    "        ### Optimize\n",
    "        prediction = prediction.to(device)\n",
    "        prediction = torch.squeeze(prediction,0)\n",
    "        label = torch.squeeze(label,0)\n",
    "        \n",
    "        new_label = torch.zeros([label.shape[0]])\n",
    "        for l in range(label.shape[0]):\n",
    "            new_label[l] = np.argmax(label[l].cpu())\n",
    "        new_label = new_label.to(device)\n",
    "        loss = criterion(prediction, new_label.long())\n",
    "        \n",
    "        # Backward prop.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### print out statistics\n",
    "        epoch_loss += loss.item()\n",
    "        if step % print_feq == 0:\n",
    "            print('epoch:', epoch,\n",
    "                  '\\tstep:', step+1, '/', len(train_dl) + 1,\n",
    "                  '\\ttrain loss:', '{:.4f}'.format(loss.item()),\n",
    "                  '\\ttime:', '{:.4f}'.format((time.time()-time_1)*print_feq), 's')\n",
    "        torch.save(model.state_dict(), 'model_lstm_2.pth')\n",
    "    ### save best model\n",
    "    if(epoch_loss < best_loss):\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), 'model_lstm_best_loss.pth')\n",
    "    print(\"The loss for this epoch is = :\", epoch_loss/lent(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Machine_Learning/datasets/YouTubeClips_2/validation/NFxWwI0J3As_78_84.avi\n",
      "when we getfilter = : 3\n",
      "torch.Size([40, 4096])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0761, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.1460,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1150,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "#### Model Testing\n",
    "model.eval();\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "utils = Utils()\n",
    "\n",
    "video_path = 'D:/Machine_Learning/datasets/YouTubeClips_2/validation/NFxWwI0J3As_78_84.avi'\n",
    "\n",
    "#print(video_path)\n",
    "video_pre_data = utils.video_to_frames(video_path,frame_number = NUMBER_OF_FRAMES, device = 'cuda', INPUT_SIZE = INPUT_SIZE , model = model_vgg, transform = tsfm)\n",
    "print(video_pre_data.shape)\n",
    "print(video_pre_data)\n",
    "X_2  = torch.zeros([NUMBER_OF_WORDS,VOCAB_SIZE])\n",
    "\n",
    "for i in range(NUMBER_OF_WORDS):\n",
    "    if (i == 0):\n",
    "        \n",
    "        X_2[i][2] = 1\n",
    "    else:\n",
    "        X_2[i][1] = 1\n",
    "\n",
    "input_data = video_pre_data.unsqueeze(0)# pre_data[200].unsqueeze(0)\n",
    "\n",
    "final_sentence = []\n",
    "\n",
    "X_2 = X_2.unsqueeze(0)\n",
    "X_2 = X_2.to(device)\n",
    "input_data = input_data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'is', 'playing', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(NUMBER_OF_WORDS-1):\n",
    "    with torch.no_grad():\n",
    "        predicted = model(input_data, X_2)\n",
    "        predicted = predicted.squeeze(0)\n",
    "        #print(torch.argmax(predicted[i]))\n",
    "        final_sentence.append(next((key for key, value in dictionary.items() if value == torch.argmax(predicted[i])), None))\n",
    "        X_2[0][i+1][torch.argmax(predicted[i])] = 1\n",
    "        X_2[0][i+1][1] = 0\n",
    "print(final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_o1UXSxTjfo_68_80\n",
    "\n",
    "video_path = 'D:/Machine_Learning/datasets/YouTubeClips_2/validation/IAvBB2lv8iw_142_148.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
